{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extractor\n",
    "\n",
    "## Problem Statement:\n",
    "Extract keywprds whcih describes the given text better\n",
    "\n",
    "## Solution\n",
    "We will make use of BERT NLP model, which is Bidirectional Encoder Representations from Transformers, a paper published by Google AI,\n",
    "applying the bidirectional training of Transformer, a popular attention model, to language modelling.\n",
    "\n",
    "### How BERT works\n",
    "BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. The detailed workings of Transformer are described in a paper by Google.\n",
    "\n",
    "As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "#### Step 1: Extract keywords using n-grams\n",
    "\n",
    "For a given text, extract key words using n-grams, We will use _CountVectorizer_ from Scikit-Learn, it allows us to specifty the length of keywords that we require for extraction\n",
    "\n",
    "#### Step 2: Create Embeddings for both document and keywords\n",
    "\n",
    "Now we generate embeddings from both document and keywords, using BERT, pretrained using distilbert — base-nli-stsb-mean-tokens or xlm-r-distilroberta-base-paraphase-v1 as they have shown great performance in semantic similarity and paraphrase identification respectively.\n",
    "\n",
    "#### Step 3: Similarity between document and keywords\n",
    "\n",
    "Final step is to find the candidates/keywords that are most similar to the document. We assume that the most similar candidates to the document are good keywords/keyphrases for representing the document\n",
    "\n",
    "We will use Cosine similarity to calculate the similarity between the keywprds and the document.\n",
    "\n",
    "#### Step 4: Diversification\n",
    "\n",
    "Step 3 returns phrases which are very similar to other, to solve this issue we will use diversification technique\n",
    "\n",
    "#### Step 5: Diversificatoin\n",
    "\n",
    "There is a reason why similar results are returned… they best represent the document! If we were to diversify the keywords/keyphrases then they are less likely to represent the document well as a collective.\n",
    "\n",
    "Thus, the diversification of our results requires a delicate balance between the accuracy of keywords/keyphrases and the diversity between them.\n",
    "There are two algorithms that we will be using to diversify our results:\n",
    "\n",
    "1. Max Sum Similarity\n",
    "2. Maximal Marginal Relevance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.5.0.tar.gz (19 kB)\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "Collecting scikit-learn>=0.22.2\n",
      "  Downloading scikit_learn-1.0.2-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.22.3-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-12.0.0-py3-none-any.whl (224 kB)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in d:\\code\\deeplearning\\keywordextractor\\vkeyword\\lib\\site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.8.0-cp39-cp39-win_amd64.whl (36.9 MB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Downloading tokenizers-0.11.6-cp39-cp39-win_amd64.whl (3.2 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: colorama in d:\\code\\deeplearning\\keywordextractor\\vkeyword\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Collecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: six in d:\\code\\deeplearning\\keywordextractor\\vkeyword\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.16.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.0.1-cp39-cp39-win_amd64.whl (3.2 MB)\n",
      "Building wheels for collected packages: keybert, sentence-transformers\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.5.0-py3-none-any.whl size=20491 sha256=e63cf8e2d9509402b6fd99112cb1202d9f5c8e39484b14b9c77b22acb8b632f6\n",
      "  Stored in directory: c:\\users\\anuko\\appdata\\local\\pip\\cache\\wheels\\ff\\a8\\1d\\c36a71d5a2603dd7e2cabcd65b55869797ddc8229226b957ba\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120748 sha256=560e6022ae0e863a8ce844190aca1ad9fd2ea423a45e686470846f30c0c9c04e\n",
      "  Stored in directory: c:\\users\\anuko\\appdata\\local\\pip\\cache\\wheels\\2b\\11\\3b\\32a18fb9f2253b25d3d1a06f0a84e2d516e7efa19c8c71a283\n",
      "Successfully built keybert sentence-transformers\n",
      "Installing collected packages: urllib3, pyparsing, idna, charset-normalizer, certifi, typing-extensions, tqdm, requests, regex, pyyaml, packaging, numpy, joblib, filelock, click, torch, tokenizers, threadpoolctl, scipy, sacremoses, pillow, huggingface-hub, transformers, torchvision, sentencepiece, scikit-learn, nltk, commonmark, sentence-transformers, rich, keybert\n",
      "Successfully installed certifi-2021.10.8 charset-normalizer-2.0.12 click-8.0.4 commonmark-0.9.1 filelock-3.6.0 huggingface-hub-0.4.0 idna-3.3 joblib-1.1.0 keybert-0.5.0 nltk-3.7 numpy-1.22.3 packaging-21.3 pillow-9.0.1 pyparsing-3.0.7 pyyaml-6.0 regex-2022.3.15 requests-2.27.1 rich-12.0.0 sacremoses-0.0.49 scikit-learn-1.0.2 scipy-1.8.0 sentence-transformers-2.2.0 sentencepiece-0.1.96 threadpoolctl-3.1.0 tokenizers-0.11.6 torch-1.11.0 torchvision-0.12.0 tqdm-4.63.0 transformers-4.17.0 typing-extensions-4.1.1 urllib3-1.26.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'D:\\Code\\DeepLearning\\KeywordExtractor\\vKeyword\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install keybert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 125kB/s]\n",
      "Downloading: 100%|██████████| 10.2k/10.2k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 124kB/s] \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 43.3kB/s]\n",
      "Downloading: 100%|██████████| 90.9M/90.9M [00:04<00:00, 21.4MB/s]\n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 17.8kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 401kB/s] \n",
      "Downloading: 100%|██████████| 350/350 [00:00<00:00, 68.2kB/s]\n",
      "Downloading: 100%|██████████| 13.2k/13.2k [00:00<00:00, 2.61MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 233kB/s]  \n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 38.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised', 0.6676),\n",
       " ('labeled', 0.4896),\n",
       " ('learning', 0.4813),\n",
       " ('training', 0.4134),\n",
       " ('labels', 0.3947)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised', 0.6676),\n",
       " ('labeled', 0.4896),\n",
       " ('learning', 0.4813),\n",
       " ('training', 0.4134),\n",
       " ('labels', 0.3947)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised learning', 0.6779),\n",
       " ('supervised', 0.6676),\n",
       " ('signal supervised', 0.6152),\n",
       " ('in supervised', 0.6124),\n",
       " ('labeled training', 0.6013)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised learning is', 0.72),\n",
       " ('supervised learning algorithm', 0.6992),\n",
       " ('in supervised learning', 0.6624),\n",
       " ('labeled training data', 0.6125),\n",
       " ('supervised learning each', 0.6098)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning function maps', 0.5341),\n",
       " ('training data unseen', 0.1464),\n",
       " ('learning algorithm analyzes', 0.4862),\n",
       " ('machine learning task', 0.2497),\n",
       " ('supervisory signal supervised', 0.3511)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with Max sum similarity\n",
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_maxsum=True, nr_candidates=20, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised learning algorithm', 0.6992),\n",
       " ('pairs infers function', 0.1981),\n",
       " ('unseen situations reasonable', 0.2142),\n",
       " ('value called supervisory', 0.2895),\n",
       " ('class labels unseen', 0.3469)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With maximum marginal relevance\n",
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_mmr=True, diversity=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidate Keywords/Keyphrases\n",
    "In some cases, one might want to be using candidate keywords generated by other keyword algorithms or retrieved from a select list of possible keywords/keyphrases. In KeyBERT, you can easily use those candidate keywords to perform keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs.[1] It infers a\n",
    "         function from labeled training data consisting of a set of training examples.[2]\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "\n",
    "# Create candidates\n",
    "kw_extractor = yake.KeywordExtractor(top=50)\n",
    "candidates = kw_extractor.extract_keywords(doc)\n",
    "candidates = [candidate[0] for candidate in candidates]\n",
    "\n",
    "# KeyBERT init\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised learning algorithm', 0.6834),\n",
       " ('Supervised learning', 0.6658),\n",
       " ('Supervised', 0.6523),\n",
       " ('labeled training data', 0.5959),\n",
       " ('labeled training', 0.5779)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided KeyBERT\n",
    "When applying KeyBERT it automatically extracts the most related keywords to a specific document. However, there are times when stakeholders and users are looking for specific types of keywords. For example, when publishing an article on your website through contentful, you typically already know the global keywords related to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "seed_keywords = [\"information\"]\n",
    "keywords = kw_model.extract_keywords(doc, use_mmr=True, diversity=0.1, seed_keywords=seed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supervised', 0.6921),\n",
       " ('learning', 0.5493),\n",
       " ('labeled', 0.5388),\n",
       " ('data', 0.4507),\n",
       " ('training', 0.4438)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa9403e858b26db2dfb342c22e7cfbe05f3951c7a34a700aa6a68ea50cc90049"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('vKeyword': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
