{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence Transformers\n",
    "\n",
    "Sentence Transformers compute vector representations for sentences and paragraphs. The model are based on DistilBERT and perform state-of-the-art results in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.\n",
    "\n",
    "We can find sentence transformers distilbert base-nli-stsb-mean-tokens model easily in transformers python library of Hugging Face\n",
    "\n",
    "! pip install transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\DeepLearning\\KeywordExtractor\\vKeyword\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range=(1,1)\n",
    "stop_words=\"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates=count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "(50, 768)\n"
     ]
    }
   ],
   "source": [
    "print(doc_embedding.shape)\n",
    "print(candidate_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class', 'training', 'supervised', 'algorithm', 'learning']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USing transformers\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Defining the model repo\n",
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\" \n",
    "\n",
    "# Download Pytorch Model\n",
    "model= AutoModel.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Transform input tokens\n",
    "inputs = tokenizer(\"Hello World !\",return_tensors='pt')\n",
    "\n",
    "# Model output\n",
    "output=model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\DeepLearning\\KeywordExtractor\\notebooks\\POC2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000004?line=0'>1</a>\u001b[0m doc_token\u001b[39m=\u001b[39mtokenizer([doc],return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m,padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000004?line=1'>2</a>\u001b[0m doc_embedding\u001b[39m=\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdoc_token)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000004?line=3'>4</a>\u001b[0m cand_token\u001b[39m=\u001b[39mtokenizer(candidates,return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m,padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000004?line=4'>5</a>\u001b[0m cand_embedding\u001b[39m=\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcand_token)\n",
      "File \u001b[1;32md:\\Code\\DeepLearning\\KeywordExtractor\\vKeyword\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "doc_token=tokenizer([doc],return_tensors='pt',padding=True,truncation=True)\n",
    "doc_embedding=model(**doc_token)\n",
    "\n",
    "cand_token=tokenizer(candidates,return_tensors='pt',padding=True,truncation=True)\n",
    "cand_embedding=model(**cand_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\DeepLearning\\KeywordExtractor\\notebooks\\POC2.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000006?line=0'>1</a>\u001b[0m X_doc\u001b[39m=\u001b[39mdoc_embedding[\u001b[39m'\u001b[39;49m\u001b[39mlast_hidden_state\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000006?line=1'>2</a>\u001b[0m X_cand\u001b[39m=\u001b[39mcand_embedding[\u001b[39m'\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000006?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(X_doc\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,X_doc\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "X_doc=doc_embedding['last_hidden_state'].detach().numpy().transpose(1,2,0)\n",
    "X_cand=cand_embedding['last_hidden_state'].detach().numpy().transpose(1,2,0)\n",
    "print(X_doc.reshape(-1,X_doc.shape[1]).shape)\n",
    "print(X_cand.reshape(-1,X_cand.shape[1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 156 while Y.shape[1] == 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\DeepLearning\\KeywordExtractor\\notebooks\\POC2.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000005?line=3'>4</a>\u001b[0m top_n \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000005?line=4'>5</a>\u001b[0m distances \u001b[39m=\u001b[39m cosine_similarity(X_doc\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,X_doc\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]), X_cand\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,X_cand\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/DeepLearning/KeywordExtractor/notebooks/POC2.ipynb#ch0000005?line=5'>6</a>\u001b[0m keywords \u001b[39m=\u001b[39m [candidates[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m distances\u001b[39m.\u001b[39margsort()[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39mtop_n:]]\n",
      "File \u001b[1;32md:\\Code\\DeepLearning\\KeywordExtractor\\vKeyword\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1251\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1216'>1217</a>\u001b[0m \u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1217'>1218</a>\u001b[0m \n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1218'>1219</a>\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1246'>1247</a>\u001b[0m \u001b[39mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1247'>1248</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1248'>1249</a>\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1250'>1251</a>\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1252'>1253</a>\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=1253'>1254</a>\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[1;32md:\\Code\\DeepLearning\\KeywordExtractor\\vKeyword\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:181\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=174'>175</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=175'>176</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPrecomputed metric requires shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=176'>177</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(n_queries, n_indexed). Got (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=177'>178</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfor \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m indexed.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=178'>179</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=179'>180</a>\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=180'>181</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=181'>182</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=182'>183</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m while Y.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=183'>184</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/Code/DeepLearning/KeywordExtractor/vKeyword/lib/site-packages/sklearn/metrics/pairwise.py?line=185'>186</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 156 while Y.shape[1] == 5"
     ]
    }
   ],
   "source": [
    "#Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(X_doc.reshape(-1,X_doc.shape[0]), X_cand.reshape(-1,X_cand.shape[0]))\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa9403e858b26db2dfb342c22e7cfbe05f3951c7a34a700aa6a68ea50cc90049"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('vKeyword': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
